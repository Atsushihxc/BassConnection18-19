{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded\n",
      "Training Fold 1 of 40 Complete\n",
      "Training Fold 2 of 40 Complete\n",
      "Training Fold 3 of 40 Complete\n",
      "Training Fold 4 of 40 Complete\n",
      "Training Fold 5 of 40 Complete\n",
      "Training Fold 6 of 40 Complete\n",
      "Training Fold 7 of 40 Complete\n",
      "Training Fold 8 of 40 Complete\n",
      "Training Fold 9 of 40 Complete\n",
      "Training Fold 10 of 40 Complete\n",
      "Training Fold 11 of 40 Complete\n",
      "Training Fold 12 of 40 Complete\n",
      "Training Fold 13 of 40 Complete\n",
      "Training Fold 14 of 40 Complete\n",
      "Training Fold 15 of 40 Complete\n",
      "Training Fold 16 of 40 Complete\n",
      "Training Fold 17 of 40 Complete\n",
      "Training Fold 18 of 40 Complete\n",
      "Training Fold 19 of 40 Complete\n",
      "Training Fold 20 of 40 Complete\n",
      "Training Fold 21 of 40 Complete\n",
      "Training Fold 22 of 40 Complete\n",
      "Training Fold 23 of 40 Complete\n",
      "Training Fold 24 of 40 Complete\n",
      "Training Fold 25 of 40 Complete\n",
      "Training Fold 26 of 40 Complete\n",
      "Training Fold 27 of 40 Complete\n",
      "Training Fold 28 of 40 Complete\n",
      "Training Fold 29 of 40 Complete\n",
      "Training Fold 30 of 40 Complete\n",
      "Training Fold 31 of 40 Complete\n",
      "Training Fold 32 of 40 Complete\n",
      "Training Fold 33 of 40 Complete\n",
      "Training Fold 34 of 40 Complete\n",
      "Training Fold 35 of 40 Complete\n",
      "Training Fold 36 of 40 Complete\n",
      "Training Fold 37 of 40 Complete\n",
      "Training Fold 38 of 40 Complete\n",
      "Training Fold 39 of 40 Complete\n",
      "Training Fold 40 of 40 Complete\n"
     ]
    }
   ],
   "source": [
    "###### -*- coding: utf-8 -*-\n",
    "'''Sample script for Kaggle \"Energy Connections\" competition\n",
    "\n",
    "Author:       Kyle Bradbury\n",
    "Date:         September 27, 2018\n",
    "Organization: Duke University Energy Initiative\n",
    "'''\n",
    "\n",
    "'''\n",
    "Import the packages needed for classification\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.metrics as metrics\n",
    "from os import path\n",
    "plt.close()\n",
    "\n",
    "'''\n",
    "Set directory parameters\n",
    "'''\n",
    "# Set the directories for the data and the CSV files that contain ids/labels\n",
    "base_dir = '/Users/Atsushi/Documents/BassConnections/KaggleData/Kaggle_First'\n",
    "dir_train_images  = path.join(base_dir, 'train')\n",
    "dir_test_images   = path.join(base_dir, 'test')\n",
    "dir_train_labels  = path.join(base_dir, 'train.csv')\n",
    "dir_test_ids      = path.join(base_dir, 'sample_submission.csv')\n",
    "\n",
    "'''\n",
    "Include the functions used for loading, preprocessing, features extraction, \n",
    "classification, and performance evaluation\n",
    "'''\n",
    "\n",
    "def load_data(dir_data, dir_labels, training=True):\n",
    "    ''' Load each of the image files into memory \n",
    "\n",
    "    While this is feasible with a smaller dataset, for larger datasets,\n",
    "    not all the images would be able to be loaded into memory\n",
    "\n",
    "    When training=True, the labels are also loaded\n",
    "    '''\n",
    "    labels_pd = pd.read_csv(dir_labels)\n",
    "    ids       = labels_pd.id.values\n",
    "    data      = []\n",
    "    \n",
    "    for identifier in ids:\n",
    "        fname     = path.join(dir_data, identifier)\n",
    "        image     = mpl.image.imread(fname)\n",
    "        data.append(image)\n",
    "    data = np.array(data) # Convert to Numpy array\n",
    "    if training:\n",
    "        labels = labels_pd.label.values\n",
    "        return data, labels\n",
    "    else:\n",
    "        return data, ids\n",
    "    \n",
    "\n",
    "def preprocess_and_extract_features(data):\n",
    "    '''Preprocess data and extract features\n",
    "    \n",
    "    Preprocess: normalize, scale, repair\n",
    "    Extract features: transformations and dimensionality reduction\n",
    "    '''\n",
    "    # Here, we do something trivially simple: we take the average of the RGB\n",
    "    # values to produce a grey image, transform that into a vector, then\n",
    "    # extract the mean and standard deviation as features.\n",
    "    \n",
    "    # Make the image grayscale\n",
    "\n",
    "    '''\n",
    "    red = data[:,:,0]\n",
    "    vec_red = red.reshape(red.shape[0],-1)\n",
    "    red = np.mean(vec_red,axis =1)\n",
    "    green = data[:,:,1]\n",
    "    vec_green = green.reshape(green.shape[0],-1)\n",
    "    green = np.mean(vec_green,axis = 1)\n",
    "    blue = data[:,:,2]\n",
    "    vec_blue = blue.reshape(blue.shape[0],-1)\n",
    "    blue = np.mean(vec_blue,axis =1)\n",
    "   '''\n",
    "    # Vectorize the grayscale matrices\n",
    "    ndata = np.mean(data, axis=3)\n",
    "\n",
    "    vectorized_data = ndata.reshape(ndata.shape[0],-1)\n",
    "\n",
    "            \n",
    "        \n",
    "    \n",
    "    # extract the mean and standard deviation of each sample as features\n",
    "    feature_min = np.min(vectorized_data,axis=1)\n",
    "    feature_max = np.max(vectorized_data,axis=1)\n",
    "    feature_mean = np.mean(vectorized_data,axis=1)\n",
    "    feature_std  = np.std(vectorized_data,axis=1)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # Combine the extracted features into a single feature vector\n",
    "    features = np.stack((feature_min,feature_max,feature_mean,feature_std),axis = -1)\n",
    "    return features\n",
    "\n",
    "def set_classifier():\n",
    "    '''Shared function to select the classifier for both performance evaluation\n",
    "    and testing\n",
    "    '''\n",
    "    return svm.SVC(probability=True, gamma = 0.001,kernel = \"linear\",C=100)\n",
    "\n",
    "def cv_performance_assessment(X,y,k,clf):\n",
    "    '''Cross validated performance assessment\n",
    "    \n",
    "    X   = training data\n",
    "    y   = training labels\n",
    "    k   = number of folds for cross validation\n",
    "    clf = classifier to use\n",
    "    \n",
    "    Divide the training data into k folds of training and validation data. \n",
    "    For each fold the classifier will be trained on the training data and\n",
    "    tested on the validation data. The classifier prediction scores are \n",
    "    aggregated and output\n",
    "    '''\n",
    "    # Establish the k folds\n",
    "    prediction_scores = np.empty(y.shape[0],dtype='object')\n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "    i = 1\n",
    "    for train_index, val_index in kf.split(X, y):\n",
    "        # Extract the training and validation data for this fold\n",
    "        X_train, X_val   = X[train_index], X[val_index]\n",
    "        y_train          = y[train_index]\n",
    "        \n",
    "        # Train the classifier\n",
    "        X_train_features = preprocess_and_extract_features(X_train)\n",
    "        clf              = clf.fit(X_train_features,y_train)\n",
    "        \n",
    "        # Test the classifier on the validation data for this fold\n",
    "        X_val_features   = preprocess_and_extract_features(X_val)\n",
    "        cpred            = clf.predict_proba(X_val_features)\n",
    "        \n",
    "        # Save the predictions for this fold\n",
    "        prediction_scores[val_index] = cpred[:,1]\n",
    "        print('Training Fold {} of {} Complete'.format(i,k))\n",
    "        i += 1\n",
    "    return prediction_scores\n",
    "\n",
    "def plot_roc(labels, prediction_scores):\n",
    "    fpr, tpr, _ = metrics.roc_curve(labels, prediction_scores, pos_label=1)\n",
    "    auc = metrics.roc_auc_score(labels, prediction_scores)\n",
    "    legend_string = 'AUC = {:0.3f}'.format(auc)\n",
    "   \n",
    "    plt.plot([0,1],[0,1],'--', color='gray', label='Chance')\n",
    "    plt.plot(fpr, tpr, label=legend_string)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.grid(True)\n",
    "    plt.axis('square')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "'''\n",
    "Sample script for running cross-validated performance assessment\n",
    "'''\n",
    "# Set parameters for the analysis\n",
    "num_training_folds = 40\n",
    "\n",
    "# Load the data\n",
    "data, labels = load_data(dir_train_images, dir_train_labels, training=True)\n",
    "print('Data Loaded')\n",
    "\n",
    "# Choose which classifier to use\n",
    "clf = set_classifier()\n",
    "\n",
    "# Perform cross validated performance assessment\n",
    "prediction_scores = cv_performance_assessment(data,labels,num_training_folds,clf)\n",
    "\n",
    "# Compute and plot the ROC curves\n",
    "plot_roc(labels, prediction_scores)\n",
    "\n",
    "\n",
    "'''\n",
    "Sample script for producing a Kaggle submission\n",
    "'''\n",
    "\n",
    "produce_submission = True \n",
    "# Switch this to True when you're ready to create a submission for Kaggle\n",
    "\n",
    "if produce_submission:\n",
    "    # Load data, extract features, and train the classifier on the training data\n",
    "    training_data, training_labels = load_data(dir_train_images, dir_train_labels, training=True)\n",
    "    training_features              = preprocess_and_extract_features(training_data)\n",
    "    clf                            = set_classifier()\n",
    "    clf.fit(training_features,training_labels)\n",
    "\n",
    "    # Load the test data and test the classifier\n",
    "    test_data, ids = load_data(dir_test_images, dir_test_ids, training=False)\n",
    "    test_features  = preprocess_and_extract_features(test_data)\n",
    "    test_scores    = clf.predict_proba(test_features)[:,1]\n",
    "\n",
    "    # Save the predictions to a CSV file for upload to Kaggle\n",
    "    submission_file = pd.DataFrame({'id':    ids,\n",
    "                                   'score':  test_scores})\n",
    "    submission_file.to_csv('submission.csv',\n",
    "                           columns=['id','score'],\n",
    "                           index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
